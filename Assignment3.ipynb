{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvTKM2vEB+FqQ4GYbKnrTn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trilokimodi/lt2222-v23-a3/blob/main/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7b9U4R3vPAK",
        "outputId": "20ca53dd-3de8-4a5c-beb5-0be413659cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import io\n",
        "from torch import nn\n",
        "from torch.utils.data.dataset import random_split"
      ],
      "metadata": {
        "id": "2TV8Oq3avweq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As instructed the data is in data folder and I assume that CWD is one directory before data dir.\n",
        "dataset = os.path.join(os.getcwd(), 'data')  # To run in MLTGPU\n",
        "dataset = \"/content/drive/My Drive/MLSNLP/data/enron_sample\"  # To run in Colab\n",
        "dataset = \"/scratch/lt2222-v23/enron_sample\" # To run in mltgpu"
      ],
      "metadata": {
        "id": "WUEsD9XNvVsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twfYErePv4HP",
        "outputId": "9d297969-64e2-4f94-db2f-ae83c909b902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lenhart-m',\n",
              " 'donohoe-t',\n",
              " 'may-l',\n",
              " 'bailey-s',\n",
              " 'keiser-k',\n",
              " 'corman-s',\n",
              " 'panus-s',\n",
              " 'dean-c',\n",
              " 'mccarty-d',\n",
              " 'lay-k',\n",
              " 'salisbury-h',\n",
              " 'schwieger-j',\n",
              " 'saibi-e',\n",
              " 'quigley-d']"
            ]
          },
          "metadata": {},
          "execution_count": 308
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Whatever other imports you need\n",
        "\n",
        "# You can implement classes and helper functions here too.\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Train and test a model on features.\")\n",
        "    parser.add_argument(\"featurefile\", type=str, help=\"The file containing the table of instances and features.\")\n",
        "    parser.add_argument('embDim', type=str, help='Embedding dimension')\n",
        "    # Add options here for part 3 -- hidden layer and nonlinearity,\n",
        "    # and any other options you may think you want/need.  Document\n",
        "    # everything.\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(f\"Reading {args.featurefile} and embedding dimension {args.embDim}\")\n",
        "\n",
        "    # implement everything you need here\n",
        "    "
      ],
      "metadata": {
        "id": "rJq6FaPUv6sP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "95836d93-2d2a-43c4-debd-cb726edd0cc8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] featurefile\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_paths = [os.path.join(os.path.join(dataset, iClass), iText) for iClass in os.listdir(dataset) for iText in os.listdir(os.path.join(dataset, iClass))]\n",
        "labelEncoder = LabelEncoder()\n",
        "labelEncoder.fit(os.listdir(dataset))"
      ],
      "metadata": {
        "id": "ULSoexZLpFL4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "cb749264-1dc7-42b7-820d-2646b1895cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 310
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer('basic_english')"
      ],
      "metadata": {
        "id": "wOSbiGA2t2Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shelley, Brad, Matt, Lawrence, Nicholas, Chad, timothy, Stacey"
      ],
      "metadata": {
        "id": "W2OB-FdJPk2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "for iPath in dataset_paths:\n",
        "    newFile = list()\n",
        "    with io.open(iPath, encoding = 'utf-8') as fh:\n",
        "        for line in fh:\n",
        "            if line.startswith(('Message-ID', 'Mime-Version:', 'Content-Type:', 'Content-Transfer-Encoding:')):\n",
        "                pass\n",
        "            elif line.startswith(('X-From:', 'X-To:', 'X-cc:', 'X-bcc:', 'X-Folder:', 'X-Origin:', 'X-FileName:')):\n",
        "                pass\n",
        "            elif re.search('From:', line) is not None:\n",
        "                pass\n",
        "            elif re.search('To:', line) is not None:\n",
        "                pass\n",
        "            elif re.search('Date:', line) is not None:\n",
        "                pass\n",
        "            elif re.search('Sent:', line) is not None:\n",
        "                pass\n",
        "            elif re.search(' -----Original Message-----', line) is not None:\n",
        "                pass\n",
        "            else:\n",
        "                newFile.append(line)\n",
        "    fh.close()\n",
        "\n",
        "    with open(iPath + '_edit', 'w') as fh:\n",
        "        for line in newFile:\n",
        "            fh.write(line)\n",
        "    fh.close()"
      ],
      "metadata": {
        "id": "L1OtG0_kM0Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with io.open(dataset_paths[110], encoding = 'utf-8') as fh:\n",
        "    print(fh.readlines())\n",
        "fh.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sig2A2HLUmHw",
        "outputId": "aa7be627-a92a-460a-acda-5e70cb06aa17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Message-ID: <24154894.1075845210312.JavaMail.evans@thyme>\\n', 'Date: Tue, 29 May 2001 13:20:07 -0700 (PDT)\\n', 'From: matthew.lenhart@enron.com\\n', 'To: ljc76@hotmail.com\\n', 'Subject: RE: Blanchard\\n', 'Mime-Version: 1.0\\n', 'Content-Type: text/plain; charset=us-ascii\\n', 'Content-Transfer-Encoding: 7bit\\n', 'X-From: Lenhart, Matthew </O=ENRON/OU=NA/CN=RECIPIENTS/CN=MLENHAR>\\n', 'X-To: \\'\"Lawrence Centola\" <ljc76@hotmail.com>@ENRON\\' <IMCEANOTES-+22Lawrence+20Centola+22+20+3Cljc76+40hotmail+2Ecom+3E+40ENRON@ENRON.com>\\n', 'X-cc: \\n', 'X-bcc: \\n', 'X-Folder: \\\\Lenhart, Matthew\\\\Lenhart, Matthew\\\\Sent Items\\n', 'X-Origin: LENHART-M\\n', 'X-FileName: Lenhart, Matthew.pst\\n', '\\n', 'who sings that song \"harley davidson of a bitch\"?  i think you said it was elo or something.\\n', '\\n', ' -----Original Message-----\\n', 'From: \\t\"Lawrence Centola\" <ljc76@hotmail.com>@ENRON [mailto:IMCEANOTES-+22Lawrence+20Centola+22+20+3Cljc76+40hotmail+2Ecom+3E+40ENRON@ENRON.com] \\n', 'Sent:\\tThursday, May 24, 2001 8:26 PM\\n', 'To:\\tLenhart, Matthew\\n', 'Subject:\\tRE: Blanchard\\n', '\\n', '\\n', 'Thanks for asking.   Face is fine now.  Graduated today and nobody really noticed.  Did have to use a little make up though.\\n', '>From: \"Lenhart, Matthew\" \\n', '>To: \"Lawrence Centola \" \\n', '>Subject: RE: Blanchard  \\n', '>Date: Wed, 23 May 2001 15:18:38 -0500  \\n', '>  \\n', '>how is your face doing? is the swelling going down?  \\n', '>  \\n', '> > -----Original Message-----  \\n', '> > From: Lawrence Centola [mailto:ljc76@hotmail.com]  \\n', '> > Sent: Thursday, April 26, 2001 12:07 AM  \\n', '> > To: sdarrah; maziarz; socalcinephile; bcambr; chad.landry;  \\n', '> > matthew.lenhart; MMMarcantel; nicholas.danna; RCasey;  \\n', '> > timothy.blanchard; tdietz; val.generes; dural54  \\n', '> > Subject: Re: Blanchard  \\n', '> >  \\n', '> >  \\n', '> > As we all know, Blanchard was a GDI his first semester at LSU. While  \\n', '> > living in Power dorm, Blanchard once overheard one of the Pi PHi for  \\n', '> > boys that he was hanging out with say \"Hey, Dude, Sorority girls are  \\n', '> > hot!\" Blanchard, being from Paincourteville and not being what we  \\n', \"> > would call a 'master' of the English language, though that this  \\n\", '> > \\'fresh\\' Pi Phi meant \"All greek girls are good in bed.\" So what did  \\n', '> > Blanchard do. He went out and found the first GREEK girl he could,  \\n', '> > and he started dating her. After many pleasant years, (some of which  \\n', \"> > I will not mention for Tim's sake, and mine) Tim and his Greek  \\n\", \"> > Goddess are engaged. Yes, that's right. Tim took Lenhart's advice  \\n\", '> > (because we all know that Lenhart is the authority on relationships  \\n', \"> > with the opposite sex; I mean, who wouldn't take his advice) and Tim  \\n\", '> > decided to start his life sentence with a lovely girl who has bunch of  \\n', '> > friends who all give it up like it was going out o! f style.  \\n', '> >  \\n', \"> > Tim's wedding is on THE DAY OF THE MISSISSIPPI STATE GAME. For those  \\n\", '> > of you who are not Tiger fans (which seems to be most of you these  \\n', '> > days), that is Oct. 20th. I know, it is an away game, which is better  \\n', '> > than having a wedding on a home game. But some of us who are fans  \\n', '> > like to go to the out of town games. True story, here. I went to a  \\n', '> > wedding with Louise and her mother on Friday night. You have not  \\n', '> > lived until you take two dates to a wedding, one of whom is engaged to  \\n', '> > your good friend and the other whom cannot speak the language.  \\n', \"> > Anyway, after the wedding that we attended, Louise's mom asked me if I  \\n\", \"> > would prefer Tim's wedding ceremony be in either half English and half  \\n\", '> > Greek, or all in Greek. I responded \"I do not care. I will be  \\n', '> > listening to the LSU game on my headphones anyway.\" I know what some  \\n', '> > of you are saying, it is only one game. Yes, and I am sure that was  \\n', '> > the only weekend they could hav! e it since so many of us from New  \\n', '> > Orleans have been to a wedding at the Greek Cultural Center, because  \\n', '> > all of us from New Orleans have been to numerous weddings / goat  \\n', '> > roasts there. Anyway, it is a done deal, and we are all happy for the  \\n', '> > couple.  \\n', '> >  \\n', \"> > The real point of this e-mail is to organize Tim's bachelor party. We  \\n\", '> > have it planned for the weekend of September 22 (what a novel idea!!!  \\n', '> > Plan a wedding event on a weekend where there is neither an LSU event,  \\n', '> > nor Jazz Fest!!!) We are planning to go to Vegas. As per our current  \\n', '> > plans, we would leave on that Thursday (the 20th, I think) and return  \\n', \"> > on Sunday. 'Paw Paw' Blanchard heard that in September, his game of  \\n\", '> > choice really starts hitting, that of course would be the nickel  \\n', '> > slots. \\'Paw Paw\\' already has his Panama Jack hat, his \"Geaux  \\n', '> > Dinar-deaux\" T-shirt, his jams, and his black socks with brown leather  \\n', '> > sandals packed and ready. His fanny pack is stuffed with Twinkies and  \\n', '> > silver dimes. He is really fired up about the $1.99 shrimp cocktail  \\n', '> > (early bird special, of course.)  \\n', '> > The plans are to get there Thursday night (Nicky, get more drink  \\n', '> > tickets.) Thursday night we can get settled, possibly gamble a bit.  \\n', '> > On Friday, maybe play golf (although I will probably be too drunk /  \\n', '> > hung over / missing for an early tee time.) On Friday night, we can  \\n', '> > go \"clubbin\\'\", as I am sure The Tricky Texas Trio (Lenhart, Landry,  \\n', '> > and Mitch) will want to do. On Saturday, Tim wants to wake up early,  \\n', '> > get the sports section of the Advocate, drink coffee, and bitch about  \\n', '> > the Tigers. Instead, we are going to watch and bet on College  \\n', '> > Football all day.  \\n', '> > Saturday night, Tim wants to go see Sigfried and Roy. However, we are  \\n', '> > not doing that. If he wants to see gay men play with caged tigers, he  \\n', '> > can find out the next time Nesbitt tries out for the LSU cheerleading  \\n', '> > team. Instead, either SaturDay or Friday night, myself and a select  \\n', '> > group (yet to be determined, but including Lenhart if he is not doing  \\n', '> > his best Ricky Martin impression at Club C2K) will go on a recon  \\n', '> > mission to the Deja Vu. Therefore, on Saturday night, we will have  \\n', '> > entertainment in the room that some of us have seen before, and who  \\n', '> > know what is expected of them (that way Lenhart will not make the  \\n', '> > entertainment cry when he asks them to fuck his Elway blow-up doll).  \\n', '> > Remember Tim. If you use a strap on, it is not considered cheating.  \\n', '> > I am a future lawyer. I should know.  \\n', '> > This weekend is September 22. There is no excuse for you to come up  \\n', '> > with when you have this much notice. Out of money? Bullshit. Save  \\n', '> > up until then. Have to work? Bullshit. Take a vacation day.  \\n', '> > Actually, September will be the first month that I will be gainfully  \\n', '> > employed. But, I know that my boss, the Honorable G.Thomas Porteous,  \\n', '> > will let me off for one day if debauchery will be had, although I may  \\n', '> > have to put a few hundreds on black for him periodically throughout  \\n', '> > the weekend.  \\n', '> > For those of you who will be recently married, I already have an  \\n', '> > excuse for you. \"Honey, Tim came to all OUR wedding functions. The  \\n', '> > least I can do is return the favor.\"  \\n', '> > For those of you that will be married shortly after Tim, I also have  \\n', '> > an excuse. \"Baby / Schmoopy / I wish your tits were as big as Mindy/,  \\n', '> > if want other people to come to our wedding functions, then I have to  \\n', '> > attend theirs.\" For all of you single guys, no excuse.  \\n', '> > Please respond to this e-mail at your earliest convenience, or I will  \\n', \"> > see ya'll this weekend. All plans are subject to change. (Chad, if  \\n\", \"> > you want to send an e-mail to only me, hit 'Reply to sender.' If you  \\n\", \"> > hit 'Reply to all'. it sends the message to all of the RECEPIENTS of  \\n\", '> > the original message. I know that you said Law School was such a  \\n', '> > stupid idea compared to Business School, but I think you could learn  \\n', '> > from some of our lessons, such as it is better to remain silent and be  \\n', '> > thought of as a fool than to open your mouth and remove all doubt.)  \\n', '> >  \\n', '> > Hope this e-mail finds you doing well.  \\n', '> >  \\n', '> > Signed,  \\n', '> > Granola, Blarry, your Daddy, Mushroom head, Captain Nic, King, Smokey,  \\n', '> > charming drunk, your heighness, the one who never looses his emotions  \\n', '> > when he drinks, SG Nerd, Governor, lawyer, or anything else you want  \\n', '> > to call me.  \\n', '> >  \\n', '> > Get your FREE download of MSN Explorer at http://explorer.msn.com <<  \\n', '> > File: ~~DLNK0.URL >>  \\n', '> >  \\n', '><< winmail.dat >>  \\n', '\\n', 'Get your FREE download of MSN Explorer at http://explorer.msn.com << File: http://explorer.msn.com >> ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with io.open(dataset_paths[110] + '_edit', encoding = 'utf-8') as fh:\n",
        "    print(fh.readlines())\n",
        "fh.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duMGFhXOUuSs",
        "outputId": "3fc89ed9-a51e-49f7-f710-c6589a2b9730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Subject: RE: Blanchard\\n', '\\n', 'who sings that song \"harley davidson of a bitch\"?  i think you said it was elo or something.\\n', '\\n', 'Subject:\\tRE: Blanchard\\n', '\\n', '\\n', 'Thanks for asking.   Face is fine now.  Graduated today and nobody really noticed.  Did have to use a little make up though.\\n', '>Subject: RE: Blanchard  \\n', '>  \\n', '>how is your face doing? is the swelling going down?  \\n', '>  \\n', '> > matthew.lenhart; MMMarcantel; nicholas.danna; RCasey;  \\n', '> > timothy.blanchard; tdietz; val.generes; dural54  \\n', '> > Subject: Re: Blanchard  \\n', '> >  \\n', '> >  \\n', '> > As we all know, Blanchard was a GDI his first semester at LSU. While  \\n', '> > living in Power dorm, Blanchard once overheard one of the Pi PHi for  \\n', '> > boys that he was hanging out with say \"Hey, Dude, Sorority girls are  \\n', '> > hot!\" Blanchard, being from Paincourteville and not being what we  \\n', \"> > would call a 'master' of the English language, though that this  \\n\", '> > \\'fresh\\' Pi Phi meant \"All greek girls are good in bed.\" So what did  \\n', '> > Blanchard do. He went out and found the first GREEK girl he could,  \\n', '> > and he started dating her. After many pleasant years, (some of which  \\n', \"> > I will not mention for Tim's sake, and mine) Tim and his Greek  \\n\", \"> > Goddess are engaged. Yes, that's right. Tim took Lenhart's advice  \\n\", '> > (because we all know that Lenhart is the authority on relationships  \\n', \"> > with the opposite sex; I mean, who wouldn't take his advice) and Tim  \\n\", '> > decided to start his life sentence with a lovely girl who has bunch of  \\n', '> > friends who all give it up like it was going out o! f style.  \\n', '> >  \\n', \"> > Tim's wedding is on THE DAY OF THE MISSISSIPPI STATE GAME. For those  \\n\", '> > of you who are not Tiger fans (which seems to be most of you these  \\n', '> > days), that is Oct. 20th. I know, it is an away game, which is better  \\n', '> > than having a wedding on a home game. But some of us who are fans  \\n', '> > like to go to the out of town games. True story, here. I went to a  \\n', '> > wedding with Louise and her mother on Friday night. You have not  \\n', '> > lived until you take two dates to a wedding, one of whom is engaged to  \\n', '> > your good friend and the other whom cannot speak the language.  \\n', \"> > Anyway, after the wedding that we attended, Louise's mom asked me if I  \\n\", \"> > would prefer Tim's wedding ceremony be in either half English and half  \\n\", '> > Greek, or all in Greek. I responded \"I do not care. I will be  \\n', '> > listening to the LSU game on my headphones anyway.\" I know what some  \\n', '> > of you are saying, it is only one game. Yes, and I am sure that was  \\n', '> > the only weekend they could hav! e it since so many of us from New  \\n', '> > Orleans have been to a wedding at the Greek Cultural Center, because  \\n', '> > all of us from New Orleans have been to numerous weddings / goat  \\n', '> > roasts there. Anyway, it is a done deal, and we are all happy for the  \\n', '> > couple.  \\n', '> >  \\n', \"> > The real point of this e-mail is to organize Tim's bachelor party. We  \\n\", '> > have it planned for the weekend of September 22 (what a novel idea!!!  \\n', '> > Plan a wedding event on a weekend where there is neither an LSU event,  \\n', '> > nor Jazz Fest!!!) We are planning to go to Vegas. As per our current  \\n', '> > plans, we would leave on that Thursday (the 20th, I think) and return  \\n', \"> > on Sunday. 'Paw Paw' Blanchard heard that in September, his game of  \\n\", '> > choice really starts hitting, that of course would be the nickel  \\n', '> > slots. \\'Paw Paw\\' already has his Panama Jack hat, his \"Geaux  \\n', '> > Dinar-deaux\" T-shirt, his jams, and his black socks with brown leather  \\n', '> > sandals packed and ready. His fanny pack is stuffed with Twinkies and  \\n', '> > silver dimes. He is really fired up about the $1.99 shrimp cocktail  \\n', '> > (early bird special, of course.)  \\n', '> > The plans are to get there Thursday night (Nicky, get more drink  \\n', '> > tickets.) Thursday night we can get settled, possibly gamble a bit.  \\n', '> > On Friday, maybe play golf (although I will probably be too drunk /  \\n', '> > hung over / missing for an early tee time.) On Friday night, we can  \\n', '> > go \"clubbin\\'\", as I am sure The Tricky Texas Trio (Lenhart, Landry,  \\n', '> > and Mitch) will want to do. On Saturday, Tim wants to wake up early,  \\n', '> > get the sports section of the Advocate, drink coffee, and bitch about  \\n', '> > the Tigers. Instead, we are going to watch and bet on College  \\n', '> > Football all day.  \\n', '> > Saturday night, Tim wants to go see Sigfried and Roy. However, we are  \\n', '> > not doing that. If he wants to see gay men play with caged tigers, he  \\n', '> > can find out the next time Nesbitt tries out for the LSU cheerleading  \\n', '> > team. Instead, either SaturDay or Friday night, myself and a select  \\n', '> > group (yet to be determined, but including Lenhart if he is not doing  \\n', '> > his best Ricky Martin impression at Club C2K) will go on a recon  \\n', '> > mission to the Deja Vu. Therefore, on Saturday night, we will have  \\n', '> > entertainment in the room that some of us have seen before, and who  \\n', '> > know what is expected of them (that way Lenhart will not make the  \\n', '> > entertainment cry when he asks them to fuck his Elway blow-up doll).  \\n', '> > Remember Tim. If you use a strap on, it is not considered cheating.  \\n', '> > I am a future lawyer. I should know.  \\n', '> > This weekend is September 22. There is no excuse for you to come up  \\n', '> > with when you have this much notice. Out of money? Bullshit. Save  \\n', '> > up until then. Have to work? Bullshit. Take a vacation day.  \\n', '> > Actually, September will be the first month that I will be gainfully  \\n', '> > employed. But, I know that my boss, the Honorable G.Thomas Porteous,  \\n', '> > will let me off for one day if debauchery will be had, although I may  \\n', '> > have to put a few hundreds on black for him periodically throughout  \\n', '> > the weekend.  \\n', '> > For those of you who will be recently married, I already have an  \\n', '> > excuse for you. \"Honey, Tim came to all OUR wedding functions. The  \\n', '> > least I can do is return the favor.\"  \\n', '> > For those of you that will be married shortly after Tim, I also have  \\n', '> > an excuse. \"Baby / Schmoopy / I wish your tits were as big as Mindy/,  \\n', '> > if want other people to come to our wedding functions, then I have to  \\n', '> > attend theirs.\" For all of you single guys, no excuse.  \\n', '> > Please respond to this e-mail at your earliest convenience, or I will  \\n', \"> > see ya'll this weekend. All plans are subject to change. (Chad, if  \\n\", \"> > you want to send an e-mail to only me, hit 'Reply to sender.' If you  \\n\", \"> > hit 'Reply to all'. it sends the message to all of the RECEPIENTS of  \\n\", '> > the original message. I know that you said Law School was such a  \\n', '> > stupid idea compared to Business School, but I think you could learn  \\n', '> > from some of our lessons, such as it is better to remain silent and be  \\n', '> > thought of as a fool than to open your mouth and remove all doubt.)  \\n', '> >  \\n', '> > Hope this e-mail finds you doing well.  \\n', '> >  \\n', '> > Signed,  \\n', '> > Granola, Blarry, your Daddy, Mushroom head, Captain Nic, King, Smokey,  \\n', '> > charming drunk, your heighness, the one who never looses his emotions  \\n', '> > when he drinks, SG Nerd, Governor, lawyer, or anything else you want  \\n', '> > to call me.  \\n', '> >  \\n', '> > Get your FREE download of MSN Explorer at http://explorer.msn.com <<  \\n', '> > File: ~~DLNK0.URL >>  \\n', '> >  \\n', '><< winmail.dat >>  \\n', '\\n', 'Get your FREE download of MSN Explorer at http://explorer.msn.com << File: http://explorer.msn.com >> ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_paths = [iPath + '_edit' for iPath in dataset_paths]"
      ],
      "metadata": {
        "id": "lGWyeNB5Vb8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yield_tokens(paths):\n",
        "    for iPath in paths:\n",
        "        with io.open(iPath, encoding = 'utf-8') as fh:\n",
        "            for line in fh:\n",
        "                yield tokenizer(line)\n",
        "        fh.close()"
      ],
      "metadata": {
        "id": "g0L1LnyatwvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabObject = build_vocab_from_iterator(yield_tokens(dataset_paths), specials=[\"<unk>\"])\n",
        "vocabObject.set_default_index(vocabObject[\"<unk>\"])"
      ],
      "metadata": {
        "id": "51CfoztjuFtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocabObject)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jptWZ-upV6T4",
        "outputId": "676f21e1-80e1-42fb-87f8-b714f72c3aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20194"
            ]
          },
          "metadata": {},
          "execution_count": 318
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataset_paths=dataset_paths, labelEncoder=labelEncoder, tokenizer=tokenizer, vocab=vocabObject):\n",
        "        self.datasetPaths = dataset_paths\n",
        "        self.labelEncoder = labelEncoder\n",
        "        self.tokenizer=tokenizer\n",
        "        self.vocab=vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.datasetPaths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        docFilePath = self.datasetPaths[idx]\n",
        "        text_pipeline = lambda x: self.vocab(self.tokenizer(x))\n",
        "        with io.open(docFilePath, encoding = 'utf-8') as fh:\n",
        "            content = fh.readlines()\n",
        "        content = ''.join(content)\n",
        "        textTensor = torch.tensor(text_pipeline(content), dtype=torch.int64)\n",
        "        label = docFilePath.split('/')[-2]\n",
        "        label = self.labelEncoder.transform([label])\n",
        "        return textTensor, label[0]"
      ],
      "metadata": {
        "id": "PEdBvXj_4A2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_data = TextDataset()"
      ],
      "metadata": {
        "id": "SCfHT-hAkbj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size=len(vocabObject), embed_dim=50, num_class=len(labelEncoder.classes_)):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "metadata": {
        "id": "GtgJKZrytHNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_text, _label) in batch:\n",
        "         text_list.append(_text)\n",
        "         label_list.append(_label)\n",
        "         offsets.append(_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    text_list = torch.cat(text_list)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    return text_list.to(device), label_list.to(device), offsets.to(device)"
      ],
      "metadata": {
        "id": "GbpODnZbHPzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TextClassificationModel().to(device)"
      ],
      "metadata": {
        "id": "3w-1QxFvwAFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (text, label, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predicted_label = model(text, offsets)\n",
        "        loss = criterion(predicted_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | accuracy {:8.3f}'.format(epoch, idx, len(dataloader), total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (text, label, offsets) in enumerate(dataloader):\n",
        "            predicted_label = model(text, offsets)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "metadata": {
        "id": "J5JhCg9p0vMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(data = dataset_paths, columns=['X'])\n",
        "df['y'] = df['X'].apply(lambda x: x.split('/')[-2])"
      ],
      "metadata": {
        "id": "8505uAKy1WKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_prop = int(len(complete_data) * 0.80)\n",
        "train_data, test_data = random_split(complete_data, [train_prop, len(complete_data) - train_prop])"
      ],
      "metadata": {
        "id": "xyHDviZJ2pvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_dataset = to_map_style_dataset(train_data)\n",
        "test_dataset = to_map_style_dataset(test_data)\n",
        "num_train = int(len(train_data) * 0.95)\n",
        "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_data) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid accuracy {:8.3f} '.format(epoch, time.time() - epoch_start_time, accu_val))\n",
        "    print('-' * 59)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWz4BOJ_1Isz",
        "outputId": "1d837ec0-05e0-4023-d088-d735d5ef3e1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time:  0.36s | valid accuracy    0.600 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time:  0.38s | valid accuracy    0.746 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time:  0.33s | valid accuracy    0.823 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time:  0.34s | valid accuracy    0.823 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time:  0.34s | valid accuracy    0.877 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time:  0.33s | valid accuracy    0.885 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time:  0.34s | valid accuracy    0.892 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time:  0.36s | valid accuracy    0.900 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time:  0.32s | valid accuracy    0.900 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time:  0.33s | valid accuracy    0.892 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ten5eqNBK0sA",
        "outputId": "6114c3b6-95e8-4618-aec9-5f1aae0b27a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Whatever other imports you need\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Convert directories into table.\")\n",
        "    parser.add_argument(\"inputdir\", type=str, help=\"The root of the author directories.\")\n",
        "    parser.add_argument(\"outputfile\", type=str, help=\"The name of the output file containing the table of instances.\")\n",
        "    parser.add_argument(\"dims\", type=int, help=\"The output feature dimensions.\")\n",
        "    parser.add_argument(\"--test\", \"-T\", dest=\"testsize\", type=int, default=\"20\", help=\"The percentage (integer) of instances to label as test.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"Reading {}...\".format(args.inputdir))\n",
        "    # Do what you need to read the documents here.\n",
        "\n",
        "    print(\"Constructing table with {} feature dimensions and {}% test instances...\".format(args.dims, args.testsize))\n",
        "    # Build the table here.\n",
        "    \n",
        "    print(\"Writing to {}...\".format(args.outputfile))\n",
        "    # Write the table out here.\n",
        "\n",
        "    print(\"Done!\")\n",
        "    "
      ],
      "metadata": {
        "id": "vcs0_Kn0q6gb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "eeb8a438-ca66-4b9f-9966-5e736605c9f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--test TESTSIZE] inputdir outputfile dims\n",
            "ipykernel_launcher.py: error: the following arguments are required: outputfile, dims\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    }
  ]
}