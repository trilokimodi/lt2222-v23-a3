{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHPrqmr0851Q/xBvG8Ntx0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trilokimodi/lt2222-v23-a3/blob/main/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7b9U4R3vPAK"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import io\n",
        "from torch import nn\n",
        "from torch.utils.data.dataset import random_split"
      ],
      "metadata": {
        "id": "2TV8Oq3avweq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As instructed the data is in data folder and I assume that CWD is one directory before data dir.\n",
        "dataset = os.path.join(os.getcwd(), 'data')  # To run in MLTGPU\n",
        "dataset = \"/content/drive/My Drive/MLSNLP/data/enron_sample\"  # To run in Colab\n",
        "#  dataset = \"/scratch/lt2222-v23/enron_sample\" # To run in mltgpu"
      ],
      "metadata": {
        "id": "WUEsD9XNvVsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(dataset)"
      ],
      "metadata": {
        "id": "twfYErePv4HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "    "
      ],
      "metadata": {
        "id": "rJq6FaPUv6sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_paths = [os.path.join(os.path.join(dataset, iClass), iText) for iClass in os.listdir(dataset) for iText in os.listdir(os.path.join(dataset, iClass))]\n",
        "labelEncoder = LabelEncoder()\n",
        "labelEncoder.fit(os.listdir(dataset))"
      ],
      "metadata": {
        "id": "ULSoexZLpFL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer('basic_english')"
      ],
      "metadata": {
        "id": "wOSbiGA2t2Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "for iPath in dataset_paths:\n",
        "    newFile = list()\n",
        "    with io.open(iPath, encoding = 'utf-8') as fh:\n",
        "        for line in fh:\n",
        "            if line.startswith(('Message-ID', 'Mime-Version:', 'Content-Type:', 'Content-Transfer-Encoding:')):\n",
        "                pass\n",
        "            elif line.startswith(('X-From:', 'X-To:', 'X-cc:', 'X-bcc:', 'X-Folder:', 'X-Origin:', 'X-FileName:')):\n",
        "                pass\n",
        "            elif re.search('From:', line) is not None:\n",
        "                pass\n",
        "            elif re.search('To:', line) is not None:\n",
        "                pass\n",
        "            elif re.search('Date:', line) is not None:\n",
        "                pass\n",
        "            elif re.search('Sent:', line) is not None:\n",
        "                pass\n",
        "            elif re.search(' -----Original Message-----', line) is not None:\n",
        "                pass\n",
        "            else:\n",
        "                newFile.append(line)\n",
        "    fh.close()\n",
        "\n",
        "    with open(iPath + '_edit', 'w') as fh:\n",
        "        for line in newFile:\n",
        "            fh.write(line)\n",
        "    fh.close()"
      ],
      "metadata": {
        "id": "L1OtG0_kM0Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with io.open(dataset_paths[110], encoding = 'utf-8') as fh:\n",
        "    print(fh.readlines())\n",
        "fh.close()"
      ],
      "metadata": {
        "id": "sig2A2HLUmHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with io.open(dataset_paths[110] + '_edit', encoding = 'utf-8') as fh:\n",
        "    print(fh.readlines())\n",
        "fh.close()"
      ],
      "metadata": {
        "id": "duMGFhXOUuSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_paths = [iPath + '_edit' for iPath in dataset_paths]"
      ],
      "metadata": {
        "id": "lGWyeNB5Vb8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yield_tokens(paths):\n",
        "    for iPath in paths:\n",
        "        with io.open(iPath, encoding = 'utf-8') as fh:\n",
        "            for line in fh:\n",
        "                yield tokenizer(line)\n",
        "        fh.close()"
      ],
      "metadata": {
        "id": "g0L1LnyatwvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabObject = build_vocab_from_iterator(yield_tokens(dataset_paths), specials=[\"<unk>\"])\n",
        "vocabObject.set_default_index(vocabObject[\"<unk>\"])"
      ],
      "metadata": {
        "id": "51CfoztjuFtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocabObject)"
      ],
      "metadata": {
        "id": "jptWZ-upV6T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataset_paths=dataset_paths, labelEncoder=labelEncoder, tokenizer=tokenizer, vocab=vocabObject):\n",
        "        self.datasetPaths = dataset_paths\n",
        "        self.labelEncoder = labelEncoder\n",
        "        self.tokenizer=tokenizer\n",
        "        self.vocab=vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.datasetPaths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        docFilePath = self.datasetPaths[idx]\n",
        "        text_pipeline = lambda x: self.vocab(self.tokenizer(x))\n",
        "        with io.open(docFilePath, encoding = 'utf-8') as fh:\n",
        "            content = fh.readlines()\n",
        "        content = ''.join(content)\n",
        "        textTensor = torch.tensor(text_pipeline(content), dtype=torch.int64)\n",
        "        label = docFilePath.split('/')[-2]\n",
        "        label = self.labelEncoder.transform([label])\n",
        "        return textTensor, label[0]"
      ],
      "metadata": {
        "id": "PEdBvXj_4A2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_data = TextDataset()"
      ],
      "metadata": {
        "id": "SCfHT-hAkbj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size=len(vocabObject), embed_dim=50, num_class=len(labelEncoder.classes_)):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "metadata": {
        "id": "GtgJKZrytHNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_text, _label) in batch:\n",
        "         text_list.append(_text)\n",
        "         label_list.append(_label)\n",
        "         offsets.append(_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    text_list = torch.cat(text_list)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    return text_list.to(device), label_list.to(device), offsets.to(device)"
      ],
      "metadata": {
        "id": "GbpODnZbHPzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TextClassificationModel().to(device)"
      ],
      "metadata": {
        "id": "3w-1QxFvwAFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (text, label, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predicted_label = model(text, offsets)\n",
        "        loss = criterion(predicted_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | accuracy {:8.3f}'.format(epoch, idx, len(dataloader), total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (text, label, offsets) in enumerate(dataloader):\n",
        "            predicted_label = model(text, offsets)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "metadata": {
        "id": "J5JhCg9p0vMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(data = dataset_paths, columns=['X'])\n",
        "df['y'] = df['X'].apply(lambda x: x.split('/')[-2])"
      ],
      "metadata": {
        "id": "8505uAKy1WKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_prop = int(len(complete_data) * 0.80)\n",
        "train_data, test_data = random_split(complete_data, [train_prop, len(complete_data) - train_prop])"
      ],
      "metadata": {
        "id": "xyHDviZJ2pvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_dataset = to_map_style_dataset(train_data)\n",
        "test_dataset = to_map_style_dataset(test_data)\n",
        "num_train = int(len(train_data) * 0.95)\n",
        "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_data) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid accuracy {:8.3f} '.format(epoch, time.time() - epoch_start_time, accu_val))\n",
        "    print('-' * 59)"
      ],
      "metadata": {
        "id": "JWz4BOJ_1Isz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "metadata": {
        "id": "ten5eqNBK0sA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}